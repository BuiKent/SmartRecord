# üé§ Whisper Models Integration Guide - Complete Implementation

> **Last Updated**: 2025-01-21  
> **Purpose**: Complete guide for integrating Whisper.cpp models into Android apps for audio transcription  
> **Target Audience**: Android developers implementing offline speech-to-text

---

## üìã Table of Contents

1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Setup & Dependencies](#setup--dependencies)
4. [Architecture](#architecture)
5. [Implementation Guide](#implementation-guide)
6. [Transcript Post-Processing](#-transcript-post-processing)
7. [Model Management](#model-management)
8. [Logging & Monitoring](#logging--monitoring)
9. [Testing](#testing)
10. [Troubleshooting](#troubleshooting)
11. [Best Practices](#best-practices)

---

## üìä Overview

### What is Whisper?

Whisper is OpenAI's automatic speech recognition (ASR) system that provides:
- **High accuracy**: ~85-95% accuracy
- **Auto punctuation**: Output includes punctuation automatically
- **Multi-language**: Supports 100+ languages (we use English)
- **Offline**: 100% on-device processing (no internet required)
- **Privacy**: Audio never leaves device

### Why Use Whisper?

| Feature | Whisper | Server STT |
|---------|---------|------------|
| Accuracy | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (~85%) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (~95%) |
| Offline | ‚úÖ Yes | ‚ùå No |
| Privacy | ‚úÖ Yes | ‚ùå No |
| Cost | ‚úÖ Free | üí∞ Paid |
| Punctuation | ‚úÖ Auto | ‚úÖ Auto |
| Model Size | ~75MB (tiny.en) | N/A |

### Model Selection

**Recommended: `tiny.en`**
- Size: ~75MB
- Accuracy: ~85%
- Speed: ~2-3x audio duration
- Memory: ~200-300MB RAM
- Best for: Mobile devices, English-only apps

**Other models:**
- `base.en` (~150MB): Better accuracy, slower
- `small.en` (~500MB): Too large for mobile
- `medium.en` (~1.5GB): Too large for mobile

---

## üîß Prerequisites

### Required Tools

1. **Android Studio** (latest version)
2. **Android NDK** (version 25.2.9519653 or newer)
3. **CMake** (3.10 or newer)
4. **Git** (to clone whisper.cpp)

### Required Knowledge

- Kotlin/Java Android development
- JNI basics (for native library integration)
- Coroutines (for async operations)
- Dependency Injection (Hilt/Dagger)

---

## üì¶ Setup & Dependencies

### 1. Clone Whisper.cpp

```bash
# Clone whisper.cpp repository (in parent directory of your project)
cd /path/to/your/project
cd ..
git clone https://github.com/ggerganov/whisper.cpp.git
```

**Directory structure:**
```
your-project/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îî‚îÄ‚îÄ main/
‚îÇ           ‚îî‚îÄ‚îÄ cpp/
‚îÇ               ‚îú‚îÄ‚îÄ CMakeLists.txt
‚îÇ               ‚îî‚îÄ‚îÄ whisper_jni.cpp
‚îî‚îÄ‚îÄ whisper.cpp/  (cloned here)
    ‚îú‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ include/
    ‚îî‚îÄ‚îÄ ggml/
```

### 2. Add Dependencies

**`app/build.gradle.kts`:**

```kotlin
android {
    // ... existing config ...
    
    // NDK configuration
    ndkVersion = "25.2.9519653"
    
    defaultConfig {
        // ... existing config ...
        
        externalNativeBuild {
            cmake {
                cppFlags += "-std=c++17"
                arguments += listOf(
                    "-DANDROID_STL=c++_shared",
                    "-DGGML_OPENMP=OFF"  // Disable OpenMP for Android
                )
            }
        }
        
        ndk {
            abiFilters += listOf("arm64-v8a", "armeabi-v7a", "x86", "x86_64")
        }
    }
    
    externalNativeBuild {
        cmake {
            path = file("src/main/cpp/CMakeLists.txt")
            version = "3.22.1"
        }
    }
}

dependencies {
    // ... existing dependencies ...
    
    // OkHttp for model download
    implementation("com.squareup.okhttp3:okhttp:4.12.0")
    
    // Hilt for dependency injection
    implementation("com.google.dagger:hilt-android:2.48")
    kapt("com.google.dagger:hilt-compiler:2.48")
}
```

### 3. Create CMakeLists.txt

**`app/src/main/cpp/CMakeLists.txt`:**

```cmake
cmake_minimum_required(VERSION 3.10)

project(whisper)

cmake_policy(SET CMP0079 NEW)
set(CMAKE_CXX_STANDARD 17)

# Path to whisper.cpp (adjust based on your directory structure)
set(WHISPER_LIB_DIR ${CMAKE_SOURCE_DIR}/../../../../whisper.cpp)

if(NOT EXISTS "${WHISPER_LIB_DIR}")
    message(FATAL_ERROR "Whisper.cpp directory not found at: ${WHISPER_LIB_DIR}")
endif()

message(STATUS "Whisper.cpp directory: ${WHISPER_LIB_DIR}")

# Source files
set(SOURCE_FILES
    ${WHISPER_LIB_DIR}/src/whisper.cpp
    ${CMAKE_SOURCE_DIR}/whisper_jni.cpp
)

find_library(LOG_LIB log)

# Build library
add_library(whisper SHARED ${SOURCE_FILES})

target_compile_definitions(whisper PUBLIC GGML_USE_CPU)
target_compile_definitions(whisper PRIVATE WHISPER_VERSION="1.5.4")

# Optimizations
if (NOT ${CMAKE_BUILD_TYPE} STREQUAL "Debug")
    target_compile_options(whisper PRIVATE -O3)
else
    target_compile_options(whisper PRIVATE -O0 -g)
endif()

# Disable OpenMP for Android
set(GGML_OPENMP OFF CACHE BOOL "ggml: use OpenMP" FORCE)

include(FetchContent)
FetchContent_Declare(ggml SOURCE_DIR ${WHISPER_LIB_DIR}/ggml)
FetchContent_MakeAvailable(ggml)

# Link libraries
target_link_libraries(whisper 
    ${LOG_LIB} 
    android 
    ggml
    c++_shared
)

# Include directories
include_directories(${WHISPER_LIB_DIR})
include_directories(${WHISPER_LIB_DIR}/src)
include_directories(${WHISPER_LIB_DIR}/include)
include_directories(${WHISPER_LIB_DIR}/ggml/include)
include_directories(${WHISPER_LIB_DIR}/ggml/src)
```

### 4. Create JNI Wrapper

**`app/src/main/cpp/whisper_jni.cpp`:**

```cpp
#include <jni.h>
#include <string>
#include <vector>
#include <android/log.h>
#include "whisper.h"

#define LOG_TAG "WhisperJNI"
#define LOGI(...) __android_log_print(ANDROID_LOG_INFO, LOG_TAG, __VA_ARGS__)
#define LOGE(...) __android_log_print(ANDROID_LOG_ERROR, LOG_TAG, __VA_ARGS__)

extern "C" {

JNIEXPORT jlong JNICALL
Java_com_yourname_smartrecorder_data_stt_WhisperEngine_initModel(
    JNIEnv *env, jobject thiz, jstring modelPath) {
    
    if (modelPath == nullptr) {
        LOGE("Model path is null");
        return 0;
    }
    
    const char *path = env->GetStringUTFChars(modelPath, nullptr);
    if (path == nullptr) {
        LOGE("Failed to get model path string");
        return 0;
    }
    
    LOGI("Loading model from: %s", path);
    
    struct whisper_context *ctx = whisper_init_from_file_with_params(
        path,
        whisper_context_default_params()
    );
    
    if (ctx == nullptr) {
        LOGE("Failed to load model from: %s", path);
        env->ReleaseStringUTFChars(modelPath, path);
        return 0;
    }
    
    env->ReleaseStringUTFChars(modelPath, path);
    LOGI("Model loaded successfully");
    return reinterpret_cast<jlong>(ctx);
}

JNIEXPORT jstring JNICALL
Java_com_yourname_smartrecorder_data_stt_WhisperEngine_transcribeAudio(
    JNIEnv *env, jobject thiz, jlong modelPtr, jshortArray audioData, jint sampleRate) {
    
    if (modelPtr == 0 || audioData == nullptr) {
        LOGE("Invalid parameters");
        return env->NewStringUTF("");
    }
    
    jsize length = env->GetArrayLength(audioData);
    if (length == 0) {
        return env->NewStringUTF("");
    }
    
    jshort *samples = env->GetShortArrayElements(audioData, nullptr);
    if (samples == nullptr) {
        return env->NewStringUTF("");
    }
    
    // Convert to float array (Whisper requires float)
    std::vector<float> pcmf32(length);
    for (int i = 0; i < length; i++) {
        pcmf32[i] = samples[i] / 32768.0f;
    }
    
    env->ReleaseShortArrayElements(audioData, samples, JNI_ABORT);
    
    struct whisper_context *ctx = reinterpret_cast<struct whisper_context*>(modelPtr);
    if (ctx == nullptr) {
        return env->NewStringUTF("");
    }
    
    // Configure Whisper parameters
    struct whisper_full_params params = whisper_full_default_params(WHISPER_SAMPLING_GREEDY);
    params.print_progress = false;
    params.translate = false;
    params.language = "en";
    params.n_threads = 4;
    params.no_context = true;
    params.single_segment = false;
    
    // Run transcription
    if (whisper_full(ctx, params, pcmf32.data(), pcmf32.size()) != 0) {
        LOGE("Failed to transcribe audio");
        return env->NewStringUTF("");
    }
    
    // Build JSON result with timestamps
    int n_segments = whisper_full_n_segments(ctx);
    std::string json = "{\"segments\":[";
    
    for (int i = 0; i < n_segments; i++) {
        const char *text = whisper_full_get_segment_text(ctx, i);
        if (text == nullptr) continue;
        
        int64_t t0 = whisper_full_get_segment_t0(ctx, i);
        int64_t t1 = whisper_full_get_segment_t1(ctx, i);
        
        double start = t0 * 0.01;  // Convert to seconds
        double end = t1 * 0.01;
        
        // Escape JSON
        std::string escaped_text;
        for (const char *p = text; *p != '\0'; p++) {
            if (*p == '"') escaped_text += "\\\"";
            else if (*p == '\\') escaped_text += "\\\\";
            else if (*p == '\n') escaped_text += "\\n";
            else escaped_text += *p;
        }
        
        if (i > 0) json += ",";
        json += "{\"text\":\"" + escaped_text + "\",\"start\":" + 
                std::to_string(start) + ",\"end\":" + std::to_string(end) + "}";
    }
    
    json += "]}";
    return env->NewStringUTF(json.c_str());
}

JNIEXPORT void JNICALL
Java_com_yourname_smartrecorder_data_stt_WhisperEngine_freeModel(
    JNIEnv *env, jobject thiz, jlong modelPtr) {
    
    if (modelPtr == 0) return;
    
    struct whisper_context *ctx = reinterpret_cast<struct whisper_context*>(modelPtr);
    if (ctx != nullptr) {
        whisper_free(ctx);
    }
}

} // extern "C"
```

**Important:** Package name is `com.yourname.smartrecorder` (converted to `com_yourname_smartrecorder` in JNI).

---

## üèóÔ∏è Architecture

### Component Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Your App                        ‚îÇ
‚îÇ  (ViewModel/UI Layer)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      WhisperAudioTranscriber            ‚îÇ
‚îÇ  (High-level transcription interface)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Audio    ‚îÇ    ‚îÇ WhisperModel      ‚îÇ
‚îÇ Converter‚îÇ    ‚îÇ Provider          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Whisper     ‚îÇ
                    ‚îÇ Engine      ‚îÇ
                    ‚îÇ (JNI)       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Whisper.cpp ‚îÇ
                    ‚îÇ (Native)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ GGML Model   ‚îÇ
                    ‚îÇ (tiny.en)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Module Responsibilities

1. **WhisperModelManager**: Downloads, verifies, and caches GGML model
2. **WhisperModelProvider**: Loads and caches model instance (singleton)
3. **AudioConverter**: Converts audio formats ‚Üí WAV PCM 16kHz mono
4. **WhisperEngine**: JNI wrapper to call Whisper.cpp native functions
5. **WhisperAudioTranscriber**: High-level interface integrating all components

---

## üíª Implementation Guide

### 1. WhisperEngine (JNI Interface)

**`app/src/main/java/com/yourname/smartrecorder/data/stt/WhisperEngine.kt`:**

```kotlin
package com.yourname.smartrecorder.data.stt

import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import org.json.JSONObject
import org.json.JSONArray
import java.io.File
import java.io.FileInputStream
import java.nio.ByteBuffer
import java.nio.ByteOrder
import javax.inject.Inject
import javax.inject.Singleton

@Singleton
class WhisperEngine @Inject constructor() {

    companion object {
        private const val TAG = "WhisperEngine"
        private const val WAV_HEADER_SIZE = 44
        private const val SAMPLE_RATE = 16000
        private const val INVALID_MODEL_PTR = 0L
        
        init {
            try {
                System.loadLibrary("whisper")
                android.util.Log.d(TAG, "Native library loaded")
            } catch (e: UnsatisfiedLinkError) {
                android.util.Log.e(TAG, "Failed to load native library", e)
            }
        }
    }
    
    // JNI functions
    external fun initModel(modelPath: String): Long
    external fun transcribeAudio(modelPtr: Long, audioData: ShortArray, sampleRate: Int): String
    external fun freeModel(modelPtr: Long)
    
    data class WhisperSegment(
        val text: String,
        val start: Double,
        val end: Double
    )
    
    suspend fun transcribe(
        modelPtr: Long,
        wavFile: File,
        onProgress: (Int) -> Unit = {}
    ): List<WhisperSegment> = withContext(Dispatchers.IO) {
        
        if (modelPtr == INVALID_MODEL_PTR) {
            throw IllegalStateException("Invalid model pointer")
        }
        
        val pcmDataSize = maxOf(0, wavFile.length() - WAV_HEADER_SIZE)
        if (pcmDataSize <= 0) {
            throw IllegalStateException("Invalid WAV file")
        }
        
        val pcmData = mutableListOf<Short>()
        
        FileInputStream(wavFile).use { stream ->
            stream.skip(WAV_HEADER_SIZE.toLong())
            
            val buffer = ByteArray(8192)
            while (true) {
                val read = stream.read(buffer)
                if (read <= 0) break
                
                val byteBuffer = ByteBuffer.wrap(buffer, 0, read)
                byteBuffer.order(ByteOrder.LITTLE_ENDIAN)
                
                while (byteBuffer.remaining() >= 2) {
                    pcmData.add(byteBuffer.short)
                }
            }
        }
        
        val audioSamples = pcmData.toShortArray()
        onProgress(50)
        
        val jsonResult = transcribeAudio(modelPtr, audioSamples, SAMPLE_RATE)
        onProgress(100)
        
        parseWhisperJson(jsonResult)
    }
    
    suspend fun initModelFromPath(modelPath: String): Long = withContext(Dispatchers.IO) {
        val modelPtr = initModel(modelPath)
        if (modelPtr == INVALID_MODEL_PTR) {
            throw IllegalStateException("Failed to initialize model")
        }
        modelPtr
    }
    
    private fun parseWhisperJson(jsonResult: String): List<WhisperSegment> {
        try {
            val json = JSONObject(jsonResult)
            val segmentsArray = json.getJSONArray("segments")
            val segments = mutableListOf<WhisperSegment>()
            
            for (i in 0 until segmentsArray.length()) {
                val segmentObj = segmentsArray.getJSONObject(i)
                val text = segmentObj.getString("text").trim()
                val start = segmentObj.getDouble("start")
                val end = segmentObj.getDouble("end")
                
                if (text.isNotBlank()) {
                    segments.add(WhisperSegment(text, start, end))
                }
            }
            
            return segments
        } catch (e: Exception) {
            android.util.Log.e(TAG, "Failed to parse JSON", e)
            return listOf(WhisperSegment(jsonResult.trim(), 0.0, 0.0))
        }
    }
}
```

### 2. WhisperModelManager (Download & Cache)

**`app/src/main/java/com/yourname/smartrecorder/data/stt/WhisperModelManager.kt`:**

```kotlin
package com.yourname.smartrecorder.data.stt

import android.content.Context
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import okhttp3.OkHttpClient
import okhttp3.Request
import java.io.File
import java.io.FileOutputStream
import java.io.IOException
import javax.inject.Inject
import javax.inject.Singleton

@Singleton
class WhisperModelManager @Inject constructor(
    @param:dagger.hilt.android.qualifiers.ApplicationContext private val context: Context
) {
    
    private val client = OkHttpClient.Builder()
        .connectTimeout(30, java.util.concurrent.TimeUnit.SECONDS)
        .readTimeout(300, java.util.concurrent.TimeUnit.SECONDS)
        .writeTimeout(300, java.util.concurrent.TimeUnit.SECONDS)
        .callTimeout(600, java.util.concurrent.TimeUnit.SECONDS)
        .retryOnConnectionFailure(true)
        .build()
    
    private val modelDir = File(context.filesDir, "whisper-models")
    private val modelName = "ggml-tiny.en.bin"
    private val modelFile = File(modelDir, modelName)
    
    companion object {
        private const val TAG = "WhisperModelManager"
        private const val MODEL_SIZE = 75L * 1024 * 1024 // 75MB
        private const val MODEL_SIZE_TOLERANCE = 0.05 // 5%
        
        private val MODEL_URLS = listOf(
            // Primary URL
            "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.en.bin",
            // Fallback URLs
            "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.en.bin?download=true",
            "https://github.com/ggerganov/whisper.cpp/releases/download/v1.5.4/ggml-tiny.en.bin"
        )
    }
    
    suspend fun downloadModel(
        onProgress: (Int) -> Unit = {}
    ) = withContext(Dispatchers.IO) {
        
        // Check if already downloaded
        if (modelFile.exists() && isModelValid(modelFile)) {
            android.util.Log.d(TAG, "Model already exists")
            onProgress(100)
            return@withContext
        }
        
        modelDir.mkdirs()
        val tempFile = File(context.cacheDir, "$modelName.tmp")
        
        // Try each URL
        for ((urlIndex, url) in MODEL_URLS.withIndex()) {
            try {
                android.util.Log.d(TAG, "Downloading from URL ${urlIndex + 1}/${MODEL_URLS.size}")
                
                val downloadedFile = downloadFile(url, tempFile) { progress ->
                    onProgress(progress)
                }
                
                // Verify model
                if (!isModelValid(downloadedFile)) {
                    throw IllegalStateException("Model verification failed")
                }
                
                // Move to final location
                if (modelFile.exists()) {
                    modelFile.delete()
                }
                downloadedFile.renameTo(modelFile)
                
                if (!modelFile.exists()) {
                    downloadedFile.copyTo(modelFile, overwrite = true)
                    downloadedFile.delete()
                }
                
                android.util.Log.d(TAG, "Model downloaded successfully")
                onProgress(100)
                return@withContext
                
            } catch (e: Exception) {
                android.util.Log.e(TAG, "Download failed from URL $urlIndex", e)
                tempFile.delete()
                
                if (urlIndex == MODEL_URLS.size - 1) {
                    throw IllegalStateException("All download URLs failed", e)
                }
            }
        }
    }
    
    private suspend fun downloadFile(
        url: String,
        outputFile: File,
        onProgress: (Int) -> Unit
    ): File = withContext(Dispatchers.IO) {
        
        val request = Request.Builder().url(url).build()
        val response = client.newCall(request).execute()
        
        if (!response.isSuccessful) {
            throw IllegalStateException("HTTP ${response.code}")
        }
        
        val totalBytes = response.body?.contentLength() ?: 0L
        if (totalBytes <= 0) {
            throw IllegalStateException("Invalid content length")
        }
        
        outputFile.parentFile?.mkdirs()
        var downloadedBytes = 0L
        
        onProgress(0)
        
        response.body?.byteStream()?.use { input ->
            FileOutputStream(outputFile).use { output ->
                val buffer = ByteArray(8192)
                var bytesRead: Int
                
                while (input.read(buffer).also { bytesRead = it } != -1) {
                    output.write(buffer, 0, bytesRead)
                    downloadedBytes += bytesRead
                    
                    if (totalBytes > 0) {
                        val progress = ((downloadedBytes * 100) / totalBytes).toInt()
                        onProgress(progress)
                    }
                }
            }
        }
        
        onProgress(100)
        outputFile
    }
    
    private fun isModelValid(modelFile: File): Boolean {
        if (!modelFile.exists() || !modelFile.isFile) {
            return false
        }
        
        val fileSize = modelFile.length()
        val minSize = (MODEL_SIZE * (1 - MODEL_SIZE_TOLERANCE)).toLong()
        val maxSize = (MODEL_SIZE * (1 + MODEL_SIZE_TOLERANCE)).toLong()
        
        return fileSize in minSize..maxSize
    }
    
    fun isModelDownloaded(): Boolean {
        return modelFile.exists() && isModelValid(modelFile)
    }
    
    fun getModelPath(): String = modelFile.absolutePath
    
    suspend fun deleteModel() = withContext(Dispatchers.IO) {
        if (modelFile.exists()) {
            modelFile.delete()
        }
    }
}
```

### 3. WhisperModelProvider (Model Loading)

**`app/src/main/java/com/yourname/smartrecorder/data/stt/WhisperModelProvider.kt`:**

```kotlin
package com.yourname.smartrecorder.data.stt

import android.content.Context
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import java.io.File
import javax.inject.Inject
import javax.inject.Singleton

@Singleton
class WhisperModelProvider @Inject constructor(
    @param:dagger.hilt.android.qualifiers.ApplicationContext private val context: Context,
    private val modelManager: WhisperModelManager,
    private val engine: WhisperEngine
) {
    private var cachedModelPtr: Long? = null
    
    companion object {
        private const val TAG = "WhisperModelProvider"
        private const val INVALID_MODEL_PTR = 0L
    }
    
    suspend fun getModel(): Long = withContext(Dispatchers.IO) {
        // Return cached model if available
        cachedModelPtr?.let {
            if (it != INVALID_MODEL_PTR) {
                return@withContext it
            }
        }
        
        // Check if model exists
        if (!modelManager.isModelDownloaded()) {
            throw IllegalStateException(
                "Model not found. Please download first using WhisperModelManager.downloadModel()"
            )
        }
        
        // Load model
        val modelPath = modelManager.getModelPath()
        val modelPtr = engine.initModelFromPath(modelPath)
        
        if (modelPtr == INVALID_MODEL_PTR) {
            throw IllegalStateException("Failed to load model")
        }
        
        cachedModelPtr = modelPtr
        android.util.Log.d(TAG, "Model loaded successfully")
        modelPtr
    }
    
    suspend fun freeModel() = withContext(Dispatchers.IO) {
        cachedModelPtr?.let { modelPtr ->
            if (modelPtr != INVALID_MODEL_PTR) {
                engine.freeModel(modelPtr)
            }
            cachedModelPtr = null
        }
    }
    
    fun isModelReady(): Boolean {
        return modelManager.isModelDownloaded()
    }
    
    fun clearCache() {
        cachedModelPtr = null
    }
}
```

### 4. AudioConverter (Audio Format Conversion)

**`app/src/main/java/com/yourname/smartrecorder/data/stt/AudioConverter.kt`:**

```kotlin
package com.yourname.smartrecorder.data.stt

import android.content.Context
import android.media.MediaCodec
import android.media.MediaExtractor
import android.media.MediaFormat
import android.net.Uri
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import java.io.File
import java.io.FileInputStream
import java.io.FileOutputStream
import java.nio.ByteBuffer
import java.nio.ByteOrder
import javax.inject.Inject
import javax.inject.Singleton

/**
 * AudioConverter: Converts audio files to WAV PCM 16kHz mono format
 * 
 * Supported Formats: MP3, M4A, WAV, OGG, FLAC ‚Üí WAV PCM 16kHz mono 16-bit
 * 
 * Output Format:
 * - Sample Rate: 16kHz (Whisper requirement)
 * - Channels: Mono (Whisper requirement)
 * - Bit Depth: 16-bit PCM
 * - Format: WAV with proper RIFF header
 */
@Singleton
class AudioConverter @Inject constructor(
    @param:dagger.hilt.android.qualifiers.ApplicationContext private val context: Context
) {
    companion object {
        private const val TAG = "AudioConverter"
        private const val SAMPLE_RATE = 16000 // Whisper requirement: 16kHz
        private const val CHANNELS = 1 // Whisper requirement: Mono
        private const val BIT_DEPTH = 16 // Whisper requirement: 16-bit
    }
    
    /**
     * Convert audio file to WAV format (16kHz, mono, 16-bit PCM)
     * Uses Android MediaCodec for real conversion
     */
    suspend fun convertToWav(
        uri: Uri,
        onProgress: (Int) -> Unit = {}
    ): File = withContext(Dispatchers.IO) {
        val outputFile = File(context.cacheDir, "input_converted_${System.currentTimeMillis()}.wav")
        var extractor: MediaExtractor? = null
        
        try {
            onProgress(5)
            
            // Step 1: Create MediaExtractor and set data source
            extractor = MediaExtractor()
            extractor.setDataSource(context, uri, null)
            
            // Step 2: Find audio track
            var audioTrackIndex = -1
            var audioFormat: MediaFormat? = null
            
            for (i in 0 until extractor.trackCount) {
                val format = extractor.getTrackFormat(i)
                val mime = format.getString(MediaFormat.KEY_MIME) ?: continue
                
                if (mime.startsWith("audio/")) {
                    audioTrackIndex = i
                    audioFormat = format
                    break
                }
            }
            
            if (audioTrackIndex == -1 || audioFormat == null) {
                throw IllegalStateException("No audio track found in file")
            }
            
            onProgress(15)
            
            // Step 3: Get audio properties
            val inputSampleRate = audioFormat.getInteger(MediaFormat.KEY_SAMPLE_RATE)
            val inputChannels = audioFormat.getInteger(MediaFormat.KEY_CHANNEL_COUNT)
            val mimeType = audioFormat.getString(MediaFormat.KEY_MIME) ?: ""
            
            // Step 4: Select track
            extractor.selectTrack(audioTrackIndex)
            
            // Step 5: Create MediaCodec decoder
            var codec: MediaCodec? = null
            var pcmOutputStream: FileOutputStream? = null
            var tempPcmFile: File? = null
            
            try {
                codec = MediaCodec.createDecoderByType(mimeType)
                codec.configure(audioFormat, null, null, 0)
                codec.start()
                
                // Create temp file for PCM data
                tempPcmFile = File(context.cacheDir, "temp_pcm_${System.currentTimeMillis()}.raw")
                pcmOutputStream = FileOutputStream(tempPcmFile)
                
                var totalBytes = 0L
                var sawInputEOS = false
                var sawOutputEOS = false
                val timeoutUs = 10000L // 10ms timeout
                
                onProgress(20)
                
                // Step 6: Decode audio using MediaCodec
                while (!sawOutputEOS) {
                    // Feed encoded data to decoder
                    if (!sawInputEOS) {
                        val inputBufferIndex = codec.dequeueInputBuffer(timeoutUs)
                        if (inputBufferIndex >= 0) {
                            val inputBuffer = codec.getInputBuffer(inputBufferIndex)
                            if (inputBuffer != null) {
                                val sampleSize = extractor.readSampleData(inputBuffer, 0)
                                
                                if (sampleSize < 0) {
                                    codec.queueInputBuffer(
                                        inputBufferIndex, 0, 0, 0,
                                        MediaCodec.BUFFER_FLAG_END_OF_STREAM
                                    )
                                    sawInputEOS = true
                                } else {
                                    val presentationTimeUs = extractor.sampleTime
                                    codec.queueInputBuffer(
                                        inputBufferIndex, 0, sampleSize,
                                        presentationTimeUs, 0
                                    )
                                    extractor.advance()
                                }
                            }
                        }
                    }
                    
                    // Get decoded PCM data
                    val bufferInfo = MediaCodec.BufferInfo()
                    val outputBufferIndex = codec.dequeueOutputBuffer(bufferInfo, timeoutUs)
                    
                    if (outputBufferIndex >= 0) {
                        if ((bufferInfo.flags and MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                            sawOutputEOS = true
                        }
                        
                        if (bufferInfo.size > 0) {
                            val outputBuffer = codec.getOutputBuffer(outputBufferIndex)
                            if (outputBuffer != null) {
                                val pcmChunk = ByteArray(bufferInfo.size)
                                outputBuffer.position(bufferInfo.offset)
                                outputBuffer.limit(bufferInfo.offset + bufferInfo.size)
                                outputBuffer.get(pcmChunk)
                                
                                pcmOutputStream?.write(pcmChunk)
                                totalBytes += bufferInfo.size
                                
                                val progress = 20 + ((totalBytes * 60) / (totalBytes + 1024 * 1024)).toInt()
                                onProgress(progress.coerceAtMost(80))
                            }
                        }
                        
                        codec.releaseOutputBuffer(outputBufferIndex, false)
                    }
                }
                
                pcmOutputStream?.close()
                pcmOutputStream = null
                
                // Cleanup MediaCodec
                codec?.stop()
                codec?.release()
                codec = null
                
                if (totalBytes == 0L || tempPcmFile == null || !tempPcmFile.exists() || tempPcmFile.length() == 0L) {
                    tempPcmFile?.delete()
                    throw IllegalStateException("No PCM data decoded from audio file")
                }
                
                onProgress(85)
                
                // Step 7: Read PCM data from file
                val rawPcmData = tempPcmFile.readBytes()
                tempPcmFile.delete()
                
                // Step 8: Convert to mono if needed
                val monoPcmData = if (inputChannels > 1) {
                    convertToMono(rawPcmData, inputChannels)
                } else {
                    rawPcmData
                }
                
                // Step 9: Resample to 16kHz if needed
                val convertedPcm = if (inputSampleRate != SAMPLE_RATE) {
                    // Use AudioResampler (you need to implement this or use a library)
                    // For simplicity, assume resampling is done here
                    resampleAudio(monoPcmData, inputSampleRate, SAMPLE_RATE)
                } else {
                    monoPcmData
                }
                
                // Step 10: Write WAV file with proper header
                writeWavFile(outputFile, convertedPcm, SAMPLE_RATE, CHANNELS, BIT_DEPTH)
                
                onProgress(100)
                outputFile
                
            } finally {
                pcmOutputStream?.close()
                codec?.stop()
                codec?.release()
                tempPcmFile?.delete()
            }
        } catch (e: Exception) {
            outputFile.delete()
            throw IllegalStateException("Failed to convert audio file: ${e.message}", e)
        } finally {
            extractor?.release()
        }
    }
    
    /**
     * Convert multi-channel audio to mono by averaging channels
     */
    private fun convertToMono(pcmData: ByteArray, inputChannels: Int): ByteArray {
        if (inputChannels == 1) {
            return pcmData
        }
        
        val totalSamples = pcmData.size / 2
        val samplesPerChannel = totalSamples / inputChannels
        val monoData = ByteArray(samplesPerChannel * 2)
        
        for (i in 0 until samplesPerChannel) {
            var sum = 0
            for (ch in 0 until inputChannels) {
                val sampleIndex = (i * inputChannels + ch) * 2
                if (sampleIndex + 1 < pcmData.size) {
                    val low = pcmData[sampleIndex].toInt() and 0xFF
                    val high = pcmData[sampleIndex + 1].toInt() and 0xFF
                    val sample = (low or (high shl 8))
                    val signedSample = if (sample > 32767) sample - 65536 else sample
                    sum += signedSample
                }
            }
            val avgSample = (sum / inputChannels).coerceIn(-32768, 32767)
            val monoIndex = i * 2
            monoData[monoIndex] = (avgSample and 0xFF).toByte()
            monoData[monoIndex + 1] = ((avgSample shr 8) and 0xFF).toByte()
        }
        
        return monoData
    }
    
    /**
     * Resample audio to target sample rate
     * Note: This is a simplified version. For production, use a proper resampling library
     */
    private fun resampleAudio(pcmData: ByteArray, fromRate: Int, toRate: Int): ByteArray {
        if (fromRate == toRate) {
            return pcmData
        }
        
        // Simple linear interpolation resampling
        val ratio = fromRate.toDouble() / toRate.toDouble()
        val inputSamples = pcmData.size / 2
        val outputSamples = (inputSamples / ratio).toInt()
        val outputData = ByteArray(outputSamples * 2)
        
        for (i in 0 until outputSamples) {
            val srcIndex = (i * ratio).toInt()
            if (srcIndex * 2 + 1 < pcmData.size) {
                val low = pcmData[srcIndex * 2].toInt() and 0xFF
                val high = pcmData[srcIndex * 2 + 1].toInt() and 0xFF
                val sample = (low or (high shl 8))
                val signedSample = if (sample > 32767) sample - 65536 else sample
                
                outputData[i * 2] = (signedSample and 0xFF).toByte()
                outputData[i * 2 + 1] = ((signedSample shr 8) and 0xFF).toByte()
            }
        }
        
        return outputData
    }
    
    /**
     * Write PCM data to WAV file with proper RIFF header
     */
    private fun writeWavFile(file: File, pcmData: ByteArray, sampleRate: Int, channels: Int, bitDepth: Int) {
        FileOutputStream(file).use { output ->
            val dataSize = pcmData.size
            val fileSize = 36 + dataSize
            
            // WAV header
            output.write("RIFF".toByteArray())
            output.write(intToByteArray(fileSize))
            output.write("WAVE".toByteArray())
            
            output.write("fmt ".toByteArray())
            output.write(intToByteArray(16))
            output.write(shortToByteArray(1)) // PCM
            output.write(shortToByteArray(channels))
            output.write(intToByteArray(sampleRate))
            output.write(intToByteArray(sampleRate * channels * bitDepth / 8))
            output.write(shortToByteArray(channels * bitDepth / 8))
            output.write(shortToByteArray(bitDepth))
            
            output.write("data".toByteArray())
            output.write(intToByteArray(dataSize))
            output.write(pcmData)
        }
    }
    
    private fun intToByteArray(value: Int): ByteArray {
        return byteArrayOf(
            (value and 0xFF).toByte(),
            ((value shr 8) and 0xFF).toByte(),
            ((value shr 16) and 0xFF).toByte(),
            ((value shr 24) and 0xFF).toByte()
        )
    }
    
    private fun shortToByteArray(value: Int): ByteArray {
        return byteArrayOf(
            (value and 0xFF).toByte(),
            ((value shr 8) and 0xFF).toByte()
        )
    }
}
```

**Note:** The resampling function above is simplified. For production use, consider using a proper audio resampling library like:
- **Android AudioResampler** (built-in, but requires native code)
- **Sonic** (C library, needs JNI wrapper)
- **SimpleAudioResampler** (Kotlin library)

### 5. WhisperAudioTranscriber (High-level Interface)

**`app/src/main/java/com/yourname/smartrecorder/data/stt/WhisperAudioTranscriber.kt`:**

```kotlin
package com.yourname.smartrecorder.data.stt

import android.content.Context
import android.net.Uri
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import java.io.File
import javax.inject.Inject
import javax.inject.Singleton

// Import post-processor
import com.yourname.smartrecorder.data.stt.WhisperPostProcessor
import com.yourname.smartrecorder.data.stt.PostProcessingOptions

interface AudioTranscriber {
    suspend fun transcribeFile(uri: Uri, onProgress: (Int) -> Unit): String
}

@Singleton
class WhisperAudioTranscriber @Inject constructor(
    @param:dagger.hilt.android.qualifiers.ApplicationContext private val context: Context,
    private val converter: AudioConverter,
    private val modelProvider: WhisperModelProvider,
    private val engine: WhisperEngine
) : AudioTranscriber {
    
    companion object {
        private const val TAG = "WhisperAudioTranscriber"
    }
    
    override suspend fun transcribeFile(
        uri: Uri,
        onProgress: (Int) -> Unit
    ): String = withContext(Dispatchers.IO) {
        
        var tempFile: File? = null
        
        try {
            // Stage 1: Load model (0-10%)
            onProgress(0)
            val modelPtr = modelProvider.getModel()
            onProgress(10)
            
            // Stage 2: Convert audio to WAV (10-30%)
            tempFile = converter.convertToWav(uri) { conversionProgress ->
                onProgress(10 + (conversionProgress * 20 / 100))
            }
            onProgress(30)
            
            // Stage 3: Transcribe (30-95%)
            val segments = engine.transcribe(modelPtr, tempFile) { transcriptionProgress ->
                onProgress(30 + (transcriptionProgress * 65 / 100))
            }
            
            // Stage 4: Post-process with heuristics + timestamps (95-100%)
            onProgress(95)
            val processedTranscript = WhisperPostProcessor.processWithTimestamps(
                segments,
                PostProcessingOptions(
                    useQuestionRule = true,        // Question-based speaker detection
                    useTimeGap = true,             // Time-gap speaker detection
                    processVoiceCommands = true,    // Process voice commands
                    removeFillers = true            // Remove filler words
                )
            )
            
            onProgress(100)
            android.util.Log.d(TAG, "Transcription completed: ${processedTranscript.length} chars")
            processedTranscript
            
        } catch (e: Exception) {
            android.util.Log.e(TAG, "Transcription failed", e)
            throw e
        } finally {
            tempFile?.delete()
        }
    }
}
```

### 6. Dependency Injection Setup

**`app/src/main/java/com/yourname/smartrecorder/di/AppModule.kt`:**

```kotlin
package com.yourname.smartrecorder.di

import android.content.Context
import com.yourname.smartrecorder.data.stt.*
import dagger.Module
import dagger.Provides
import dagger.hilt.InstallIn
import dagger.hilt.android.qualifiers.ApplicationContext
import dagger.hilt.components.SingletonComponent
import javax.inject.Singleton

@Module
@InstallIn(SingletonComponent::class)
object AppModule {
    
    @Provides
    @Singleton
    fun provideWhisperModelManager(
        @ApplicationContext context: Context
    ): WhisperModelManager {
        return WhisperModelManager(context)
    }
    
    @Provides
    @Singleton
    fun provideWhisperEngine(): WhisperEngine {
        return WhisperEngine()
    }
    
    @Provides
    @Singleton
    fun provideWhisperModelProvider(
        @ApplicationContext context: Context,
        modelManager: WhisperModelManager,
        engine: WhisperEngine
    ): WhisperModelProvider {
        return WhisperModelProvider(context, modelManager, engine)
    }
    
    @Provides
    @Singleton
    fun provideAudioConverter(
        @ApplicationContext context: Context
    ): AudioConverter {
        return AudioConverter(context)
    }
    
    @Provides
    @Singleton
    fun provideWhisperAudioTranscriber(
        @ApplicationContext context: Context,
        converter: AudioConverter,
        modelProvider: WhisperModelProvider,
        engine: WhisperEngine
    ): WhisperAudioTranscriber {
        return WhisperAudioTranscriber(context, converter, modelProvider, engine)
    }
}
```

### 7. Integration with GenerateTranscriptUseCase

**Important:** `WhisperAudioTranscriber` returns `String`, but `GenerateTranscriptUseCase` needs `List<TranscriptSegment>`. We need to convert.

**`app/src/main/java/com/yourname/smartrecorder/domain/usecase/GenerateTranscriptUseCase.kt`:**

Update the existing `GenerateTranscriptUseCase` to use Whisper:

```kotlin
package com.yourname.smartrecorder.domain.usecase

import com.yourname.smartrecorder.core.logging.AppLogger
import com.yourname.smartrecorder.core.logging.AppLogger.TAG_TRANSCRIPT
import com.yourname.smartrecorder.core.logging.AppLogger.TAG_USECASE
import com.yourname.smartrecorder.data.stt.WhisperAudioTranscriber
import com.yourname.smartrecorder.data.stt.WhisperEngine
import com.yourname.smartrecorder.domain.model.Recording
import com.yourname.smartrecorder.domain.model.TranscriptSegment
import com.yourname.smartrecorder.domain.repository.TranscriptRepository
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import java.io.File
import android.net.Uri
import android.content.ContentResolver
import javax.inject.Inject

/**
 * Use case for generating transcript from audio file using Whisper.
 */
class GenerateTranscriptUseCase @Inject constructor(
    private val transcriptRepository: TranscriptRepository,
    private val transcriber: WhisperAudioTranscriber
) {
    suspend operator fun invoke(
        recording: Recording,
        onProgress: (Int) -> Unit = {}
    ): List<TranscriptSegment> = withContext(Dispatchers.IO) {
        val startTime = System.currentTimeMillis()
        AppLogger.logUseCase(TAG_USECASE, "GenerateTranscriptUseCase", "Starting", 
            mapOf("recordingId" to recording.id, "filePath" to recording.filePath))
        
        val audioFile = File(recording.filePath)
        if (!audioFile.exists()) {
            AppLogger.e(TAG_TRANSCRIPT, "Audio file not found -> path: %s", null, recording.filePath)
            throw IllegalStateException("Audio file not found: ${recording.filePath}")
        }
        
        val fileSize = audioFile.length()
        val estimatedDurationMs = recording.durationMs
        AppLogger.d(TAG_TRANSCRIPT, "Audio file info -> size: %d bytes, duration: %d ms", 
            fileSize, estimatedDurationMs)
        
        try {
            // Convert File path to Uri
            val uri = android.net.Uri.fromFile(audioFile)
            
            // Transcribe using Whisper
            val transcriptText = transcriber.transcribeFile(uri) { progress ->
                onProgress(progress)
                AppLogger.d(TAG_TRANSCRIPT, "Transcription progress: %d%%", progress)
            }
            
            // Parse transcript text into segments
            // Note: WhisperAudioTranscriber returns processed text with speaker labels
            // We need to extract segments from WhisperEngine.WhisperSegment list
            // For now, we'll create a single segment or parse the text
            
            // TODO: Better approach - modify WhisperAudioTranscriber to return segments
            // For now, create segments from the full transcript
            val segments = createSegmentsFromText(
                recordingId = recording.id,
                transcriptText = transcriptText,
                durationMs = estimatedDurationMs
            )
            
            AppLogger.d(TAG_TRANSCRIPT, "Generated %d transcript segments", segments.size)
            
            // Save to repository
            AppLogger.d(TAG_TRANSCRIPT, "Saving transcript segments to database")
            transcriptRepository.saveTranscriptSegments(recording.id, segments)
            
            val duration = System.currentTimeMillis() - startTime
            AppLogger.logUseCase(TAG_USECASE, "GenerateTranscriptUseCase", "Completed", 
                mapOf("recordingId" to recording.id, "segments" to segments.size, "duration" to "${duration}ms"))
            AppLogger.logPerformance(TAG_TRANSCRIPT, "GenerateTranscriptUseCase", duration, 
                "segments=${segments.size}, fileSize=${fileSize}bytes")
            
            segments
            
        } catch (e: Exception) {
            AppLogger.e(TAG_TRANSCRIPT, "Transcription failed", e)
            throw e
        }
    }
    
    /**
     * Create segments from transcript text.
     * This is a temporary solution - ideally WhisperAudioTranscriber should return segments directly.
     */
    private fun createSegmentsFromText(
        recordingId: String,
        transcriptText: String,
        durationMs: Long
    ): List<TranscriptSegment> {
        if (transcriptText.isBlank()) {
            return emptyList()
        }
        
        // Split by speaker labels or newlines
        val lines = transcriptText.split("\n\n", "\n").filter { it.isNotBlank() }
        
        if (lines.isEmpty()) {
            return emptyList()
        }
        
        // Calculate segment duration
        val segmentDuration = durationMs / lines.size.coerceAtLeast(1)
        
        return lines.mapIndexed { index, line ->
            // Remove speaker labels if present
            val cleanText = line.replace(Regex("\\[Speaker \\d+\\]:\\s*"), "").trim()
            
            // Detect if it's a question
            val isQuestion = cleanText.trim().endsWith("?")
            
            TranscriptSegment(
                id = index.toLong(),
                recordingId = recordingId,
                startTimeMs = index * segmentDuration,
                endTimeMs = (index + 1) * segmentDuration,
                text = cleanText,
                isQuestion = isQuestion
            )
        }
    }
}
```

**Better Approach:** Modify `WhisperAudioTranscriber` to return segments directly:

```kotlin
// In WhisperAudioTranscriber.kt - add new method
suspend fun transcribeFileToSegments(
    uri: Uri,
    onProgress: (Int) -> Unit
): List<WhisperEngine.WhisperSegment> = withContext(Dispatchers.IO) {
    // ... same as transcribeFile but return segments instead of String
    val segments = engine.transcribe(modelPtr, tempFile) { transcriptionProgress ->
        onProgress(30 + (transcriptionProgress * 65 / 100))
    }
    
    // Post-process segments (keep as segments, don't convert to String)
    val processedSegments = WhisperPostProcessor.processSegments(
        segments,
        PostProcessingOptions(...)
    )
    
    processedSegments
}
```

Then update `GenerateTranscriptUseCase`:

```kotlin
// Use transcribeFileToSegments instead
val whisperSegments = transcriber.transcribeFileToSegments(uri) { progress ->
    onProgress(progress)
}

// Convert WhisperEngine.WhisperSegment to TranscriptSegment
val segments = whisperSegments.mapIndexed { index, whisperSegment ->
    TranscriptSegment(
        id = index.toLong(),
        recordingId = recording.id,
        startTimeMs = (whisperSegment.start * 1000).toLong(),
        endTimeMs = (whisperSegment.end * 1000).toLong(),
        text = whisperSegment.text,
        isQuestion = whisperSegment.text.trim().endsWith("?")
    )
}
```

---

## üìù Transcript Post-Processing

### Why Post-Processing?

Whisper output is already good, but post-processing improves:
- **Text quality**: Remove filler words, fix grammar, normalize units
- **Speaker detection**: Identify different speakers in conversations
- **Formatting**: Add line breaks, speaker labels for readability
- **Voice commands**: Process spoken commands like "new line", "comma"

### Post-Processing Pipeline

```
Raw Whisper Output (with timestamps)
    ‚Üì
1. Voice Commands Processing (optional)
    ‚Üì
2. English Heuristics (text cleaning)
    ‚Üì
3. Speaker Detection (question rule + time gap)
    ‚Üì
Final Processed Transcript
```

### Implementation

**`app/src/main/java/com/yourname/smartrecorder/data/stt/WhisperPostProcessor.kt`:**

```kotlin
package com.yourname.smartrecorder.data.stt

/**
 * WhisperPostProcessor: Post-processing for Whisper output
 * 
 * Features:
 * - English Heuristics: Text cleaning, formatting
 * - Question-based speaker detection: After question (?) ‚Üí change speaker
 * - Time-gap speaker detection: Silence > 1.5s ‚Üí change speaker
 * - Voice commands processing
 */
object WhisperPostProcessor {
    private const val TAG = "WhisperPostProcessor"
    
    // Filler words to remove
    private val FILLER_WORDS = listOf(
        "um", "uh", "er", "ah", "oh", "hum", "hmm", "hm", "eh", "huh", "mm", "mmm"
    )
    
    /**
     * Process Whisper output with English heuristics
     */
    fun processEnglishHeuristics(text: String): String {
        var processed = text.trim()
        if (processed.isBlank()) return processed
        
        // Step 1: Remove filler words
        val fillerRegex = Regex("\\b(${FILLER_WORDS.joinToString("|")})\\b", RegexOption.IGNORE_CASE)
        processed = fillerRegex.replace(processed, "")
            .replace(Regex("\\s+"), " ") // Remove extra spaces
        
        // Step 2: Remove repeated words (stuttering)
        // "the the car" ‚Üí "the car"
        processed = processed.replace(Regex("\\b(\\w+)\\s+\\1\\b", RegexOption.IGNORE_CASE), "$1")
        
        // Step 3: Grammar fixes
        // Fix "i" ‚Üí "I"
        processed = processed.replace(Regex("\\bi\\b"), "I")
        processed = processed.replace(Regex("\\bi'(m|ll|ve|d)\\b"), "I'$1")
        
        // Step 4: Normalize units & currency
        processed = processed.replace(Regex("(\\d+)\\s+percent", RegexOption.IGNORE_CASE), "$1%")
        processed = processed.replace(Regex("(\\d+)\\s+dollars", RegexOption.IGNORE_CASE), "$$$1")
        processed = processed.replace(Regex("(\\d+)\\s+pounds", RegexOption.IGNORE_CASE), "¬£$1")
        
        return processed.trim()
    }
    
    /**
     * Process voice commands
     */
    fun processVoiceCommands(text: String): String {
        val voiceCommands = mapOf(
            "new line" to "\n",
            "comma" to ",",
            "period" to ".",
            "question mark" to "?",
            "exclamation mark" to "!",
            "colon" to ":",
            "semicolon" to ";"
        )
        
        var processed = text
        voiceCommands.forEach { (command, replacement) ->
            val regex = Regex("\\b$command\\b", RegexOption.IGNORE_CASE)
            processed = regex.replace(processed, replacement)
        }
        
        return processed
    }
    
    /**
     * Process with timestamps: Combine question rule (priority) and time gap (fallback)
     * Logic: OR condition - either question mark OR time gap > 1.5s ‚Üí change speaker
     */
    fun processWithTimestamps(
        segments: List<WhisperEngine.WhisperSegment>,
        options: PostProcessingOptions = PostProcessingOptions()
    ): String {
        if (segments.isEmpty()) return ""
        
        // Step 1: Process each segment text with heuristics
        val processedSegments = segments.map { segment ->
            var processedText = segment.text.trim()
            
            // Voice commands
            if (options.processVoiceCommands) {
                processedText = processVoiceCommands(processedText)
            }
            
            // English heuristics
            processedText = processEnglishHeuristics(processedText)
            
            if (processedText.isBlank()) null
            else ProcessedSegment(processedText, segment.start, segment.end)
        }.filterNotNull()
        
        if (processedSegments.isEmpty()) return ""
        
        // Step 2: Determine speakers for each segment
        val speakerAssignments = mutableListOf<Int>()
        var currentSpeaker = 1
        var lastEndTime = 0.0
        
        processedSegments.forEachIndexed { index, segment ->
            val isQuestion = segment.text.trim().endsWith("?")
            val prevSegment = processedSegments.getOrNull(index - 1)
            val prevIsQuestion = prevSegment?.text?.trim()?.endsWith("?") ?: false
            
            // Calculate time gap (silence)
            val silenceGap = if (index > 0) segment.start - lastEndTime else 0.0
            val isLongPause = silenceGap > 1.5
            
            // Logic OR: Priority 1 = question mark, Priority 2 = time gap
            var shouldChangeSpeaker = false
            
            if (options.useQuestionRule && isQuestion) {
                // Priority 1: Question ‚Üí change speaker
                shouldChangeSpeaker = true
            } else if (options.useTimeGap && isLongPause && !prevIsQuestion) {
                // Priority 2: Time gap > 1.5s (only if not after question)
                shouldChangeSpeaker = true
            } else if (options.useQuestionRule && prevIsQuestion && !isQuestion) {
                // After question, next sentence is not question ‚Üí back to speaker 1
                shouldChangeSpeaker = true
            }
            
            // Change speaker if needed
            if (shouldChangeSpeaker && index > 0) {
                currentSpeaker = if (currentSpeaker == 1) 2 else 1
            }
            
            speakerAssignments.add(currentSpeaker)
            lastEndTime = segment.end
        }
        
        // Step 3: Build result
        val uniqueSpeakers = speakerAssignments.distinct()
        val hasMultipleSpeakers = uniqueSpeakers.size > 1
        val result = StringBuilder()
        
        if (hasMultipleSpeakers) {
            // Multiple speakers ‚Üí add labels and line breaks
            var prevSpeaker = -1
            
            processedSegments.forEachIndexed { index, segment ->
                val speaker = speakerAssignments[index]
                
                if (speaker != prevSpeaker) {
                    if (index > 0) {
                        result.append("\n\n")
                    }
                    result.append("[Speaker $speaker]: ")
                } else if (index > 0) {
                    result.append("\n")
                }
                
                result.append(segment.text.trim())
                prevSpeaker = speaker
            }
        } else {
            // Single speaker ‚Üí join all segments, no labels
            processedSegments.forEachIndexed { index, segment ->
                if (index > 0) {
                    result.append(" ")
                }
                result.append(segment.text.trim())
            }
        }
        
        return result.toString().trim()
    }
    
    private data class ProcessedSegment(
        val text: String,
        val start: Double,
        val end: Double
    )
}

/**
 * Post-processing options
 */
data class PostProcessingOptions(
    val useQuestionRule: Boolean = true,        // After question (?) ‚Üí change speaker (priority)
    val useTimeGap: Boolean = true,             // Time gap > 1.5s ‚Üí change speaker (fallback)
    val processVoiceCommands: Boolean = true,    // Process voice commands
    val removeFillers: Boolean = true            // Remove filler words (integrated in heuristics)
)
```

### Integration into WhisperAudioTranscriber

Update `WhisperAudioTranscriber` to use post-processing:

```kotlin
// In WhisperAudioTranscriber.kt

// Stage 3: Transcribe (30-95%)
val segments = engine.transcribe(modelPtr, tempFile) { transcriptionProgress ->
    onProgress(30 + (transcriptionProgress * 65 / 100))
}

// Stage 4: Post-process with heuristics + timestamps (95-100%)
onProgress(95)
val processedTranscript = WhisperPostProcessor.processWithTimestamps(
    segments,
    PostProcessingOptions(
        useQuestionRule = true,        // Question-based speaker detection
        useTimeGap = true,             // Time-gap speaker detection
        processVoiceCommands = true,    // Process voice commands
        removeFillers = true            // Remove filler words
    )
)

onProgress(100)
return processedTranscript
```

### Customization Guide

#### What to Add

1. **Language-specific heuristics** (if not English):
   ```kotlin
   fun processVietnameseHeuristics(text: String): String {
       // Vietnamese-specific rules
   }
   ```

2. **Custom filler words**:
   ```kotlin
   private val CUSTOM_FILLER_WORDS = listOf("you know", "sort of", "kind of")
   ```

3. **Custom voice commands**:
   ```kotlin
   val customCommands = mapOf(
       "paragraph" to "\n\n",
       "bullet point" to "‚Ä¢ "
   )
   ```

4. **Number normalization**:
   ```kotlin
   // "twenty five" ‚Üí "25"
   processed = processed.replace(Regex("twenty\\s+five", RegexOption.IGNORE_CASE), "25")
   ```

5. **Acronym expansion**:
   ```kotlin
   val acronyms = mapOf(
       "AI" to "Artificial Intelligence",
       "API" to "Application Programming Interface"
   )
   ```

#### What to Remove

1. **Speaker detection** (if single speaker only):
   ```kotlin
   PostProcessingOptions(
       useQuestionRule = false,
       useTimeGap = false
   )
   ```

2. **Voice commands** (if not needed):
   ```kotlin
   PostProcessingOptions(
       processVoiceCommands = false
   )
   ```

3. **Filler word removal** (if you want to keep natural speech):
   ```kotlin
   PostProcessingOptions(
       removeFillers = false
   )
   ```

#### Important Notes

1. **Preserve Whisper punctuation**: Whisper already adds punctuation, don't remove it
2. **Timestamps are crucial**: Use timestamps for accurate speaker detection
3. **Question rule priority**: Question mark detection is more reliable than time gap
4. **Single vs multiple speakers**: 
   - Single speaker: No labels, just clean text
   - Multiple speakers: Add labels and line breaks
5. **Performance**: Post-processing is fast (< 100ms for typical transcripts)
6. **Language support**: Current heuristics are English-specific, adapt for other languages

### Example Output

**Before (Raw Whisper):**
```
um hello there um i want to ask you a question? yes please go ahead. um okay so what is the answer?
```

**After (Post-processed):**
```
[Speaker 1]: Hello there, I want to ask you a question?
[Speaker 2]: Yes, please go ahead.
[Speaker 1]: Okay, so what is the answer?
```

### Testing Post-Processing

```kotlin
@Test
fun testPostProcessing() {
    val rawText = "um hello there um i want to ask you a question?"
    val processed = WhisperPostProcessor.processEnglishHeuristics(rawText)
    assertEquals("Hello there, I want to ask you a question?", processed)
}

@Test
fun testSpeakerDetection() {
    val segments = listOf(
        WhisperEngine.WhisperSegment("Hello there.", 0.0, 2.0),
        WhisperEngine.WhisperSegment("What is the answer?", 2.5, 5.0),
        WhisperEngine.WhisperSegment("The answer is 42.", 5.5, 8.0)
    )
    
    val result = WhisperPostProcessor.processWithTimestamps(
        segments,
        PostProcessingOptions(useQuestionRule = true, useTimeGap = true)
    )
    
    assertTrue(result.contains("[Speaker 1]"))
    assertTrue(result.contains("[Speaker 2]"))
}
```

---

## üì• Model Management

### Download Model

```kotlin
// In your ViewModel or Activity
class MainViewModel @Inject constructor(
    private val modelManager: WhisperModelManager
) : ViewModel() {
    
    fun downloadModel(onProgress: (Int) -> Unit) {
        viewModelScope.launch {
            try {
                modelManager.downloadModel { progress ->
                    onProgress(progress)
                }
                // Model downloaded successfully
            } catch (e: Exception) {
                // Handle error
            }
        }
    }
}
```

### Check Model Status

```kotlin
fun isModelReady(): Boolean {
    return modelProvider.isModelReady()
}
```

### Delete Model (for re-download)

```kotlin
suspend fun deleteModel() {
    modelManager.deleteModel()
    modelProvider.clearCache()
}
```

### Model Storage Location

- **Path**: `context.filesDir/whisper-models/ggml-tiny.en.bin`
- **Size**: ~75MB
- **Persistence**: Survives app updates
- **Verification**: File size check (75MB ¬± 5%)

---

## üìä Logging & Monitoring

### Logging Setup

**Create a simple logger:**

```kotlin
object ProductionLogger {
    private const val TAG_PREFIX = "Whisper"
    
    fun d(message: String) {
        android.util.Log.d(TAG_PREFIX, message)
    }
    
    fun e(message: String, throwable: Throwable? = null) {
        android.util.Log.e(TAG_PREFIX, message, throwable)
    }
    
    fun w(message: String) {
        android.util.Log.w(TAG_PREFIX, message)
    }
}
```

### Key Log Points

1. **Model Download:**
   - Start download
   - Progress updates (every 10%)
   - Download completion
   - Verification status

2. **Model Loading:**
   - Model initialization start
   - Model load time
   - Model ready status

3. **Transcription:**
   - Transcription start
   - Progress updates
   - Completion status
   - Error details

### View Logs

```bash
# Filter Whisper logs
adb logcat | grep -i whisper

# View all errors
adb logcat *:E

# View specific tag
adb logcat WhisperEngine:D WhisperModelManager:D WhisperAudioTranscriber:D
```

### Analytics Events (Optional)

```kotlin
// Track model download
analytics.logEvent("whisper_model_download", mapOf(
    "success" to true,
    "duration_ms" to downloadDuration,
    "url_index" to urlIndex
))

// Track transcription
analytics.logEvent("whisper_transcription", mapOf(
    "success" to true,
    "duration_ms" to transcriptionDuration,
    "audio_duration_ms" to audioDuration,
    "text_length" to transcript.length
))
```

---

## üß™ Testing

### Unit Tests

**`app/src/test/java/com/yourname/smartrecorder/data/stt/WhisperModelManagerTest.kt`:**

```kotlin
@Test
fun `test model validation`() {
    val validFile = File.createTempFile("test_model", ".bin")
    validFile.writeBytes(ByteArray(75 * 1024 * 1024)) // 75MB
    
    val isValid = modelManager.isModelValid(validFile)
    assertTrue(isValid)
    
    validFile.delete()
}

@Test
fun `test model path`() {
    val path = modelManager.getModelPath()
    assertTrue(path.endsWith("ggml-tiny.en.bin"))
}
```

### Integration Tests

**`app/src/androidTest/java/com/yourname/smartrecorder/data/stt/WhisperIntegrationTest.kt`:**

```kotlin
@RunWith(AndroidJUnit4::class)
class WhisperIntegrationTest {
    
    @get:Rule
    val hiltRule = HiltAndroidRule(this)
    
    @Inject
    lateinit var modelManager: WhisperModelManager
    
    @Inject
    lateinit var modelProvider: WhisperModelProvider
    
    @Inject
    lateinit var transcriber: WhisperAudioTranscriber
    
    @Before
    fun init() {
        hiltRule.inject()
    }
    
    @Test
    fun testModelDownload() = runTest {
        modelManager.downloadModel { progress ->
            assertTrue(progress in 0..100)
        }
        
        assertTrue(modelManager.isModelDownloaded())
    }
    
    @Test
    fun testTranscription() = runTest {
        // Ensure model is downloaded
        if (!modelManager.isModelDownloaded()) {
            modelManager.downloadModel {}
        }
        
        // Load model
        val modelPtr = modelProvider.getModel()
        assertNotEquals(0L, modelPtr)
        
        // Test transcription with sample audio file
        // (You need to provide a test audio file)
    }
}
```

### Manual Testing Checklist

1. **Model Download:**
   - [ ] Download starts correctly
   - [ ] Progress updates smoothly
   - [ ] Download completes successfully
   - [ ] Model file exists after download
   - [ ] Re-download skips if model exists

2. **Model Loading:**
   - [ ] Model loads successfully
   - [ ] Model loads from cache on second call
   - [ ] Error handling when model missing

3. **Transcription:**
   - [ ] Transcription works with WAV file
   - [ ] Progress updates correctly
   - [ ] Output includes punctuation
   - [ ] Timestamps are accurate
   - [ ] Error handling for invalid files

4. **Performance:**
   - [ ] Model load time < 5 seconds
   - [ ] Transcription time ~2-3x audio duration
   - [ ] Memory usage < 300MB

---

## üîß Troubleshooting

### Issue 1: Native Library Not Found

**Error:**
```
UnsatisfiedLinkError: dlopen failed: library "libwhisper.so" not found
```

**Solutions:**
1. Verify `libwhisper.so` exists in APK:
   ```bash
   unzip -l app-debug.apk | grep whisper
   ```
2. Check CMake build succeeded
3. Verify NDK version is correct
4. Clean and rebuild:
   ```bash
   ./gradlew clean
   ./gradlew assembleDebug
   ```

### Issue 2: Model Download Fails

**Error:**
```
Failed to download model from all URLs
```

**Solutions:**
1. Check internet connection
2. Verify URLs are accessible:
   ```bash
   curl -I https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.en.bin
   ```
3. Check device storage space (need > 100MB free)
4. Try fallback URLs manually

### Issue 3: Transcription Very Slow

**Symptoms:** Transcription takes > 5x audio duration

**Solutions:**
1. Normal for first run (model loading)
2. Check device performance (use newer device)
3. Consider using smaller model (tiny.en)
4. Reduce audio file length (process in chunks)

### Issue 4: App Crashes on Transcription

**Error:** App force closes during transcription

**Solutions:**
1. Check logcat for native crash:
   ```bash
   adb logcat | grep -i "fatal\|crash\|signal"
   ```
2. Verify model file integrity
3. Check memory usage (may be OOM)
4. Test with smaller audio file first

### Issue 5: CMake Build Fails

**Error:**
```
CMake Error: Whisper.cpp directory not found
```

**Solutions:**
1. Verify whisper.cpp path in CMakeLists.txt
2. Check directory structure:
   ```
   your-project/
   ‚îú‚îÄ‚îÄ app/
   ‚îî‚îÄ‚îÄ whisper.cpp/  (should be here)
   ```
3. Update path in CMakeLists.txt if needed

---

## ‚úÖ Best Practices

### 1. Model Management

- ‚úÖ **Download once**: Check if model exists before downloading
- ‚úÖ **Verify model**: Always verify file size after download
- ‚úÖ **Cache model pointer**: Reuse loaded model instance
- ‚úÖ **Handle errors gracefully**: Provide user-friendly error messages

### 2. Performance

- ‚úÖ **Lazy loading**: Load model only when needed
- ‚úÖ **Progress tracking**: Show progress for long operations
- ‚úÖ **Timeout protection**: Set timeouts for transcription
- ‚úÖ **Memory management**: Clean up temp files after use

### 3. Error Handling

- ‚úÖ **Specific errors**: Use custom error types
- ‚úÖ **User messages**: Convert technical errors to user-friendly messages
- ‚úÖ **Retry logic**: Implement retry for network operations
- ‚úÖ **Fallback options**: Provide alternative transcription methods

### 4. Logging

- ‚úÖ **Structured logging**: Use consistent log format
- ‚úÖ **Log levels**: Use appropriate log levels (D/E/W)
- ‚úÖ **Performance metrics**: Log timing information
- ‚úÖ **Error details**: Include stack traces for errors

### 5. Testing

- ‚úÖ **Unit tests**: Test individual components
- ‚úÖ **Integration tests**: Test full transcription flow
- ‚úÖ **Performance tests**: Measure transcription speed
- ‚úÖ **Error tests**: Test error handling scenarios

---

## üìö References

### Official Resources

- **Whisper.cpp GitHub**: https://github.com/ggerganov/whisper.cpp
- **Whisper.cpp Models**: https://huggingface.co/ggerganov/whisper.cpp
- **Whisper Paper**: https://arxiv.org/abs/2212.04356

### Android Resources

- **Android NDK**: https://developer.android.com/ndk
- **JNI Guide**: https://developer.android.com/training/articles/perf-jni
- **CMake Guide**: https://developer.android.com/ndk/guides/cmake

### Model Downloads

- **Primary**: https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.en.bin
- **GitHub Releases**: https://github.com/ggerganov/whisper.cpp/releases

---

## üìù License

This implementation guide is provided as-is. Whisper.cpp is licensed under MIT License.

---

**Last Updated**: 2025-01-21  
**Status**: ‚úÖ Complete Implementation Guide
